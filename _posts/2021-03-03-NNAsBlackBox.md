---
published: false
---
## A New Post

جذر المشكلة يعود لأكثر من سبب: منها تطور البرامج -خصوصًا المكتبات عالية المستوى (High-level and Abstract libraries)- والتي أصبحت جيدة بما يكفي ليغنيك عن معرفة ما يدور خلف العمليات الأساسية. مما جعل الصناديق السوداء (black boxes) السمة الأساسية للتعلم العميق وتعلم الآلة.
أضافة إلى أن تفاصيل عمل الكثير من التقنيات المؤتمتة مملوكة بحد كبير للقطاع الخاص ويتم اعتبارها أسرارًا تجارية ولا يتم الافصاح عن تفاصيلها حتى بموجب أمر قضائي. مما ترتب على ذلك غياب للشفافية والمساءلة وأدى لعواقب وخيمة. كما حدث مع برنامج COMPAS – أداة لدعم القرار تستخدمها بعض المحاكم الأمريكية لتقييم احتمال عودة المدعى عليه إلى الإجرام-. 




## المراجع
- https://www.nytimes.com/2017/06/13/opinion/how-computers-are-harming-criminal-justice.html

- Rudin, C., 2019. Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead. Nature Machine Intelligence, 1(5), pp.206-215.

- Fong, R.C. and Vedaldi, A., 2017. Interpretable explanations of black boxes by meaningful perturbation. In Proceedings of the IEEE International Conference on Computer Vision (pp. 3429-3437).

