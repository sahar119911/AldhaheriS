---
published: False
---
## A New Post
كما هو الحال مع الثوراث التكنولوجية فإن الذكاء الاصطناعي وبشكل خاص الشبكات العصبية العميقة تثير الكثير من التساؤلات والشكوك مؤخرًا خاصة مع الاتجاه المتزايد للاستفادة منها في الرعاية الصحية والعدالة الجنائية. الانتقاد الأكثر شيوعًا عند التعامل مع هذه التقنيات هو عدم قدرة البشر على فهم مايدور بين تلك الشبكات وآلية اتخاذ القرار ومقارنته بالقرار البشري. في حين يرى الكثيرون أن التطور التقني مرحب به من الجانب النظري, يرى أخرون أن الجانب العملي والتطبيقي عليه الكثير من المآخذ. خصوصًا مع غياب الشفافية والمساءلة مما أدى للكثير من العواقب الوخيمة. 

يعود جذر المشكلة لأكثر من سبب: منها تطور البرامج -خصوصًا المكتبات عالية المستوى (High-level and Abstract libraries)- والتي أصبحت جيدة بما يكفي ليغنيك عن معرفة ما يدور خلف العمليات الأساسية. مما جعل الصناديق السوداء (black boxes) السمة الأساسية للتعلم العميق وتعلم الآلة.
أضافة إلى أن تفاصيل عمل الكثير من التقنيات المؤتمتة مملوكة بحد كبير للقطاع الخاص ويتم اعتبارها أسرارًا تجارية ولا يتم الافصاح عن تفاصيلها حتى بموجب أمر قضائي. مما ترتب عليه غياب للشفافية والمساءلة  كما يحدث مع برنامج 

كان هناك العديد من النقاشات في المراجعات الأدبية حول عدالة مثل هذه الخوارزميات، والتي نشأت أساسًا من التحليل الذي أجرته مجموعة ProPublica مدعيا أن نموذج التنبؤ COMPAS – أداة لدعم القرار تستخدمها بعض المحاكم الأمريكية لتقييم احتمال عودة المدعى عليه إلى الإجرام- متحيز عنصريًا . حيث قامت هذه الأداء بحساب نتيجة تتنبأ باحتمالية ارتكاب كل منهما جريمة في المستقبل. تم تصنيف معتقل من أصل افريقي ليس لديه سجل إجرامي؛ بإنه خطر على المجتمع واحتمالية عودته للجريمة عالية. بينما تم تصنيف معتقل أبيض لديه سجل اجرامي حافل على أنه منخفض المخاطر واحتمالية عودته للجريمة ضئيلة!
 بالتالي يمكن أن يؤدي هذا النوع من الأخطاء إلى سنوات من السجن الإضافي ، أو إطلاق سراح الأفراد الخطرين أو المتطرفين في المجتمع. يلفت هذا النوع من الدراسات الانتباه إلى مدى أهمية الشفافية في اتخاذ القرارات القضائية عند استخدام الذكاء الاصطناعي أو التعلم الآلي أو الأنواع أخرى من النماذج الإحصائية. لا يمكننا تحديد ما إذا كانت هذه الاخطاء ناتجة عن أخطاء في الحساب أو أخطاء في إدخال البيانات أو أخطاء من مصدر آخر (أو حتى إذا كانت من الأساس تصنف ك أخطاء).
 




## المراجع
- https://www.nytimes.com/2017/06/13/opinion/how-computers-are-harming-criminal-justice.html

- Rudin, C., 2019. Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead. Nature Machine Intelligence, 1(5), pp.206-215.

- Fong, R.C. and Vedaldi, A., 2017. Interpretable explanations of black boxes by meaningful perturbation. In Proceedings of the IEEE International Conference on Computer Vision (pp. 3429-3437).

